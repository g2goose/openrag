---
title: Deploy OpenRAG with self-managed services
slug: /docker
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import PartialOnboarding from '@site/docs/_partial-onboarding.mdx';
import PartialPrereqCommon from '@site/docs/_partial-prereq-common.mdx';
import PartialPrereqNoScript from '@site/docs/_partial-prereq-no-script.mdx';
import PartialPrereqWindows from '@site/docs/_partial-prereq-windows.mdx';
import PartialPrereqPython from '@site/docs/_partial-prereq-python.mdx';
import PartialInstallNextSteps from '@site/docs/_partial-install-next-steps.mdx';

To manage your own OpenRAG services, deploy OpenRAG with Docker or Podman.

Use this installation method if you don't want to [use the Terminal User Interface (TUI)](/tui), or you need to run OpenRAG in an environment where using the TUI is unfeasible.

## Prerequisites

<PartialPrereqWindows />

<PartialPrereqCommon />

<PartialPrereqPython />

<PartialPrereqNoScript />

## Prepare your deployment

1. Clone the OpenRAG repository:

   ```bash
   git clone https://github.com/langflow-ai/openrag.git
   ```

2. Change to the root of the cloned repository:

   ```bash
   cd openrag
   ```

3. Install dependencies:

   ```bash
   uv sync
   ```

4. Create a `.env` file at the root of the cloned repository.

   You can create an empty file or copy the repository's [`.env.example`](https://github.com/langflow-ai/openrag/blob/main/.env.example) file.
   The example file contains some of the [OpenRAG environment variables](/reference/configuration) to get you started with configuring your deployment.

   ```bash
   cp .env.example .env
   ```

5. Edit the `.env` file to configure your deployment using [OpenRAG environment variables](/reference/configuration).
The OpenRAG Docker Compose files pull values from your `.env` file to configure the OpenRAG containers.
The following variables are required or recommended:

   * **`OPENSEARCH_PASSWORD` (Required)**: Sets the OpenSearch administrator password. It must adhere to the [OpenSearch password complexity requirements](https://docs.opensearch.org/latest/security/configuration/demo-configuration/#setting-up-a-custom-admin-password).

   * **`LANGFLOW_SUPERUSER`**: The username for the Langflow administrator user. Defaults to `admin` if not set.

   * **`LANGFLOW_SUPERUSER_PASSWORD` (Strongly recommended)**: Sets the Langflow administrator password, and determines the Langflow server's default authentication mode. If not set, the Langflow server starts without authentication enabled. For more information, see [Langflow settings](/reference/configuration#langflow-settings).

   * **`LANGFLOW_SECRET_KEY` (Strongly recommended)**: A secret encryption key for internal Langflow operations. It is recommended to [generate your own Langflow secret key](https://docs.langflow.org/api-keys-and-authentication#langflow-secret-key). If not set, Langflow generates a secret key automatically.

   * **Model provider credentials**: Provide credentials for your preferred model providers. If not set in the `.env` file, you must configure at least one provider during [application onboarding](#application-onboarding).

      * `OPENAI_API_KEY`
      * `ANTHROPIC_API_KEY`
      * `OLLAMA_ENDPOINT`
      * `WATSONX_API_KEY`
      * `WATSONX_ENDPOINT`
      * `WATSONX_PROJECT_ID`

   * **OAuth provider credentials**: To upload documents from external storage, such as Google Drive, set the required OAuth credentials for the connectors that you want to use. You can [manage OAuth credentials](/ingestion#oauth-ingestion) later, but it is recommended to configure them during initial set up so you don't have to rebuild the containers.

      * **Amazon**: Provide your AWS Access Key ID and AWS Secret Access Key with access to your S3 instance. For more information, see the AWS documentation on [Configuring access to AWS applications](https://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-applications.html).
      * **Google**: Provide your Google OAuth Client ID and Google OAuth Client Secret. You can generate these in the [Google Cloud Console](https://console.cloud.google.com/apis/credentials). For more information, see the [Google OAuth client documentation](https://developers.google.com/identity/protocols/oauth2).
      * **Microsoft**: For the Microsoft OAuth Client ID and Microsoft OAuth Client Secret, provide [Azure application registration credentials for SharePoint and OneDrive](https://learn.microsoft.com/en-us/onedrive/developer/rest-api/getting-started/app-registration?view=odsp-graph-online). For more information, see the [Microsoft Graph OAuth client documentation](https://learn.microsoft.com/en-us/onedrive/developer/rest-api/getting-started/graph-oauth).

   For more information and variables, see [Environment variables](/reference/configuration).

6. Start `docling serve` on the host machine.
   OpenRAG Docker installations require that `docling serve` is running on port 5001 on the host machine.
   This enables [Mac MLX](https://opensource.apple.com/projects/mlx/) support for document processing.

   ```bash
   uv run python scripts/docling_ctl.py start --port 5001
   ```

7. Confirm `docling serve` is running.
   ```
   uv run python scripts/docling_ctl.py status
   ```

   Make sure the response shows that `docling serve` is running, for example:
   ```bash
   Status: running
   Endpoint: http://127.0.0.1:5001
   Docs: http://127.0.0.1:5001/docs
   PID: 27746
   ```

8. Deploy OpenRAG locally with the appropriate Docker Compose file for your environment.
Both files deploy the same services.

   * [`docker-compose.yml`](https://github.com/langflow-ai/openrag/blob/main/docker-compose.yml) is an OpenRAG deployment with GPU support for accelerated AI processing. This Docker Compose file requires an NVIDIA GPU with [CUDA](https://docs.nvidia.com/cuda/) support.

      * Docker:

         ```bash
         docker compose build
         docker compose up -d
         ```

      * Podman Compose:

         ```bash
         podman compose build
         podman compose up -d
         ```

   * [`docker-compose-cpu.yml`](https://github.com/langflow-ai/openrag/blob/main/docker-compose-cpu.yml) is a CPU-only version of OpenRAG for systems without NVIDIA GPU support. Use this Docker Compose file for environments where GPU drivers aren't available.

      * Docker:

         ```bash
         docker compose -f docker-compose-cpu.yml up -d
         ```

      * Podman Compose:

         ```bash
         podman compose -f docker-compose-cpu.yml up -d
         ```

   The OpenRAG Docker Compose file starts five containers:

   | Container Name | Default address | Purpose |
   |---|---|---|
   | OpenRAG Backend | http://localhost:8000 | FastAPI server and core functionality. |
   | OpenRAG Frontend | http://localhost:3000 | React web interface for user interaction. |
   | Langflow | http://localhost:7860 | [AI workflow engine](/agents). |
   | OpenSearch | http://localhost:9200 | Datastore for [knowledge](/knowledge). |
   | OpenSearch Dashboards | http://localhost:5601 | OpenSearch database administration interface. |

9. Wait while the containers start, and then confirm all containers are running:

   * Docker Compose:

      ```bash
      docker compose ps
      ```

   * Podman Compose:

      ```bash
      podman compose ps
      ```

   If all containers are running, you can access your OpenRAG services at their addresses.

10. Access the OpenRAG frontend at `http://localhost:3000` to continue with [application onboarding](#application-onboarding).

<PartialOnboarding />

<PartialInstallNextSteps />